import asyncio
import random
import re
from datetime import datetime
from typing import Dict, List, Any, Optional, TypedDict

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from ..memory.base import Message, MemoryContext
from ..memory.hybrid_memory import HybridMemory
from ..config.settings import settings
from ..utils.prompt_loader import PromptLoader
from ..utils.time_utils import TimeUtils
from ..utils.message_controller import MessageController
from ..utils.behavioral_analyzer import BehavioralAnalyzer
from ..utils.prompt_composer import PromptComposer

class PipelineState(TypedDict):
    """State object for the pipeline - Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ñ‹Ð¹ TypedDict Ð´Ð»Ñ LangGraph"""
    user_id: str
    messages: List[Dict[str, Any]]
    meta_time: Optional[datetime]
    
    # Pipeline state
    normalized_input: str
    memory_context: str
    day_prompt: str
    behavior_prompt: str
    final_prompt: str
    llm_response: str
    processed_response: Dict[str, Any]
    
    # Behavioral Analysis
    current_strategy: str
    behavioral_analysis: Dict[str, Any]
    strategy_confidence: float
    
    # Metadata
    day_number: int
    question_count: int
    processing_start: datetime

class AgathaPipeline:
 
    
    def __init__(self):
        self.prompt_loader = PromptLoader()
        self.time_utils = TimeUtils()
        self.memory_instances: Dict[str, HybridMemory] = {}
        # MessageController Ñ‚ÐµÐ¿ÐµÑ€ÑŒ ÑÐ¾Ð·Ð´Ð°ÐµÑ‚ÑÑ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
        self.message_controllers: Dict[str, MessageController] = {}
        self.behavioral_analyzer = BehavioralAnalyzer()
        self.prompt_composer = PromptComposer()
        
        # Initialize LLM - ÐÐžÐ’Ð«Ð™ API
        if settings.OPENAI_API_KEY:
            self.llm = ChatOpenAI(
                api_key=settings.OPENAI_API_KEY,
                model=settings.LLM_MODEL,
                temperature=0.8
            )
        else:
            self.llm = None  
        
        # Build the graph - ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐ«Ð™ API
        self.graph = self._build_graph()
    
    def _build_graph(self):
        """Build the LangGraph pipeline - Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Ñ Ð½Ð¾Ð²Ñ‹Ð¼ API"""
        workflow = StateGraph(PipelineState)
        
        # Add nodes
        workflow.add_node("ingest_input", self._ingest_input)
        workflow.add_node("short_memory", self._short_memory)
        workflow.add_node("day_policy", self._day_policy)
        workflow.add_node("behavior_policy", self._behavior_policy)
        workflow.add_node("compose_prompt", self._compose_prompt)
        workflow.add_node("llm_call", self._llm_call)
        workflow.add_node("postprocess", self._postprocess)
        workflow.add_node("persist", self._persist)
        
        # Add edges with new API
        workflow.add_edge(START, "ingest_input")
        workflow.add_edge("ingest_input", "short_memory")
        workflow.add_edge("short_memory", "day_policy")
        workflow.add_edge("day_policy", "behavior_policy")
        workflow.add_edge("behavior_policy", "compose_prompt")
        workflow.add_edge("compose_prompt", "llm_call")
        workflow.add_edge("llm_call", "postprocess")
        workflow.add_edge("postprocess", "persist")
        workflow.add_edge("persist", END)
        
        return workflow.compile()
    
    async def process_chat(self, user_id: str, messages: List[Dict], meta_time: Optional[str] = None) -> Dict[str, Any]:
        """Main entry point for chat processing - Ð¿Ð¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ"""
        state: PipelineState = {
            "user_id": user_id,
            "messages": messages,
            "meta_time": None,
            "normalized_input": "",
            "memory_context": "",
            "day_prompt": "",
            "behavior_prompt": "",
            "final_prompt": "",
            "llm_response": "",
            "processed_response": {},
            "current_strategy": "caring",
            "behavioral_analysis": {},
            "strategy_confidence": 0.0,
            "day_number": 1,
            "question_count": 0,
            "processing_start": datetime.utcnow()
        }
        
        if meta_time:
            try:
                state["meta_time"] = datetime.fromisoformat(meta_time.replace('Z', '+00:00'))
            except:
                state["meta_time"] = datetime.utcnow()
        else:
            state["meta_time"] = datetime.utcnow()
        
        # Run the pipeline Ñ‡ÐµÑ€ÐµÐ· LangGraph
        result = await self.graph.ainvoke(state)
        return result["processed_response"]
    
    async def _fallback_pipeline(self, state: PipelineState) -> Dict[str, Any]:
        """Fallback pipeline Ð±ÐµÐ· LangGraph - ÐÐ• Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð•Ð¢Ð¡Ð¯!"""
        # Ð­Ñ‚Ð¾Ñ‚ Ð¼ÐµÑ‚Ð¾Ð´ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ - Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸
        # Ð’ÑÐµÐ³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ LangGraph pipeline
        raise Exception("Fallback pipeline Ð½Ðµ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ! Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ LangGraph.")
    
    async def _ingest_input(self, state: PipelineState) -> PipelineState:
        """Node 1: Normalize input and extract metadata"""
        if not state["messages"]:
            state["normalized_input"] = ""
            return state
        
        # Get the last user message
        user_messages = [msg for msg in state["messages"] if msg.get('role') == 'user']
        if user_messages:
            last_message = user_messages[-1]
            state["normalized_input"] = last_message.get('content', '').strip()
        
        # Calculate day number (simplified)
        state["day_number"] = 1  # TODO: Calculate from user profile
        
        return state
    
    async def _short_memory(self, state: PipelineState) -> PipelineState:
        """Node 2: Load and process short-term memory"""
        user_id = state["user_id"]
        
        # Get or create memory instance for user
        if user_id not in self.memory_instances:
            self.memory_instances[user_id] = HybridMemory(user_id)
        
        memory = self.memory_instances[user_id]
        
        # Add current message to memory
        if state["normalized_input"]:
            message = Message(
                role="user",
                content=state["normalized_input"],
                timestamp=state["meta_time"] or datetime.utcnow()
            )
            context = MemoryContext(
                user_id=user_id,
                day_number=state["day_number"]
            )
            await memory.add_message(message, context)
        
        # Get memory context
        state["memory_context"] = await memory.get_context(MemoryContext(
            user_id=user_id,
            day_number=state["day_number"]
        ))
        
        return state
    
    async def _day_policy(self, state: PipelineState) -> PipelineState:
        """Node 3: Apply daily scenario policy with enhanced context"""
        day_number = state["day_number"]
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð½ÐµÐ²Ð½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        state["day_prompt"] = await self.prompt_loader.get_day_prompt(day_number)
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð¸ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸
        user_id = state["user_id"]
        memory = self.memory_instances.get(user_id)
        
        if memory:
            try:
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸
                profile = await memory.get_user_profile()
                insights = await memory.get_conversation_insights()
                
                # ÐžÐ±Ð¾Ð³Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼
                time_context = self.time_utils.get_time_context(state["meta_time"])
                
                enhanced_prompt = f"""
{state["day_prompt"]}

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢ ÐžÐ¢ÐÐžÐ¨Ð•ÐÐ˜Ð™:
- Ð¡Ñ‚Ð°Ð´Ð¸Ñ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹: {insights.get('relationship_stage', 'introduction')}
- Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {insights.get('personalization_level', 0):.1f}/1.0
- ÐÐ°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {profile.get('recent_mood', 'neutral')}
- Ð›ÑŽÐ±Ð¸Ð¼Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹: {', '.join([t[0] for t in profile.get('favorite_topics', [])[:3]])}

Ð’Ð Ð•ÐœÐ•ÐÐÐžÐ™ ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢:
{time_context}

ÐŸÐÐœÐ¯Ð¢Ð¬ ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯:
{state["memory_context"][:200]}...
                """.strip()
                
                state["day_prompt"] = enhanced_prompt
                
            except Exception as e:
                print(f"Warning: Could not enhance day prompt: {e}")
        
        return state
    
    async def _behavior_policy(self, state: PipelineState) -> PipelineState:
        """Node 4: Advanced Behavioral Adaptation - Ð°Ð½Ð°Ð»Ð¸Ð· Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸"""
        user_id = state["user_id"]
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
        memory = self.memory_instances.get(user_id)
        user_profile = {}
        conversation_context = {}
        
        if memory:
            try:
                user_profile = await memory.get_user_profile()
                conversation_context = await memory.get_conversation_insights()
            except Exception as e:
                print(f"Warning: Could not get user profile: {e}")
        
        # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        behavioral_analysis = await self.behavioral_analyzer.analyze_user_behavior(
            messages=state["messages"],
            user_profile=user_profile,
            conversation_context=conversation_context
        )
        
        # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
        recommended_strategy = behavioral_analysis['recommended_strategy']
        strategy_confidence = behavioral_analysis['strategy_confidence']
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸
        state["current_strategy"] = recommended_strategy
        state["behavioral_analysis"] = behavioral_analysis
        state["strategy_confidence"] = strategy_confidence
        
        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ‹Ð±Ð¾Ñ€ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸
        print(f"ðŸŽ­ Behavioral Analysis for {user_id}:")
        print(f"   Emotion: {behavioral_analysis['dominant_emotion']} (intensity: {behavioral_analysis['emotional_intensity']:.2f})")
        print(f"   Communication: {behavioral_analysis['communication_style']}")
        print(f"   Needs: {', '.join(behavioral_analysis['relationship_needs'][:2])}")
        print(f"   Strategy: {recommended_strategy} (confidence: {strategy_confidence:.2f})")
        
        return state
    
    async def _compose_prompt(self, state: PipelineState) -> PipelineState:
        """Node 5: Advanced Prompt Composition with Behavioral Integration"""
        base_prompt = await self.prompt_loader.get_base_prompt()
        day_prompt = state["day_prompt"]
        strategy = state["current_strategy"]
        behavioral_analysis = state.get("behavioral_analysis", {})
        
        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        time_context = self.time_utils.get_time_context(state["meta_time"])
        
        context_data = {
            'time_context': time_context,
            'memory_context': state["memory_context"],
            'user_message': state["normalized_input"],
            'max_length': settings.MAX_MESSAGE_LENGTH,
            'day_number': state["day_number"]
        }
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ PromptComposer Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
        state["final_prompt"] = await self.prompt_composer.compose_final_prompt(
            base_prompt=base_prompt,
            day_prompt=day_prompt,
            strategy=strategy,
            behavioral_analysis=behavioral_analysis,
            context_data=context_data
        )
        
        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
        print(f"ðŸ“ Prompt Composition:")
        print(f"   Strategy: {strategy}")
        print(f"   Day: {state['day_number']}")
        print(f"   User emotion: {behavioral_analysis.get('dominant_emotion', 'unknown')}")
        print(f"   Prompt length: {len(state['final_prompt'])} chars")
        
        return state
    
    async def _llm_call(self, state: PipelineState) -> PipelineState:
        """Node 6: Call LLM and get response"""
        if self.llm is None:
            # Fallback response for testing Ñ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°Ð¼Ð¸
            user_input = state["normalized_input"]
            strategy = state["current_strategy"]
            day = state["day_number"]
            
            mock_responses = {
                "caring": f"ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ñ€Ð°Ð´Ð° Ñ‚ÐµÐ±Ñ Ð²Ð¸Ð´ÐµÑ‚ÑŒ ðŸ˜Š Ð¢Ñ‹ Ð½Ð°Ð¿Ð¸ÑÐ°Ð»: '{user_input}'. ÐšÐ°Ðº Ñƒ Ñ‚ÐµÐ±Ñ Ð´ÐµÐ»Ð°? Ð¯ Ñ…Ð¾Ñ‡Ñƒ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ Ñ‚ÐµÐ±Ñ!",
                "mysterious": f"Ð˜Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð¾... '{user_input}' ðŸ¤” Ð•ÑÑ‚ÑŒ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾Ðµ Ð² Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ñ‚Ñ‹ ÑÐºÐ°Ð·Ð°Ð». Ð§Ñ‚Ð¾ ÑÐºÑ€Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð·Ð° ÑÑ‚Ð¸Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸?",
                "playful": f"ÐžÐ¹-Ð¾Ð¹! '{user_input}' ðŸ˜„ Ð¢Ñ‹ Ñ‚Ð°ÐºÐ¾Ð¹ Ð·Ð°Ð±Ð°Ð²Ð½Ñ‹Ð¹! Ð¥Ð¾Ñ‡ÐµÑˆÑŒ Ð¿Ð¾Ð¸Ð³Ñ€Ð°Ñ‚ÑŒ Ð² ÑÐ»Ð¾Ð²ÐµÑÐ½Ñ‹Ðµ Ð¸Ð³Ñ€Ñ‹?",
                "reserved": f"ÐŸÐ¾Ð½Ð¸Ð¼Ð°ÑŽ. Ð¢Ñ‹ ÑÐºÐ°Ð·Ð°Ð»: '{user_input}'. Ð­Ñ‚Ð¾ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ð·Ñ€ÐµÐ½Ð¸Ñ."
            }
            
            base_response = mock_responses.get(strategy, f"ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð”ÐµÐ½ÑŒ {day}, ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ñ {strategy}. Ð¢Ñ‹ ÑÐºÐ°Ð·Ð°Ð»: '{user_input}'")
            state["llm_response"] = base_response
        else:
            try:
                # Ð ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ API OpenAI
                response = await self.llm.ainvoke([HumanMessage(content=state["final_prompt"])])
                state["llm_response"] = response.content.strip()
            except Exception as e:
                print(f"LLM call failed: {e}")
                state["llm_response"] = "Ð˜Ð·Ð²Ð¸Ð½Ð¸, Ñƒ Ð¼ÐµÐ½Ñ ÑÐµÐ¹Ñ‡Ð°Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ðµ Ñ€Ð°Ð·?"
        
        return state
    
    async def _postprocess(self, state: PipelineState) -> PipelineState:
        """Node 7: Post-process response (split, add delays, manage questions)"""
        response_text = state["llm_response"]
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
        user_id = state["user_id"]
        memory = self.memory_instances.get(user_id)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð»Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ MessageController Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        if user_id not in self.message_controllers:
            self.message_controllers[user_id] = MessageController(
                max_message_length=settings.MAX_MESSAGE_LENGTH,
                question_frequency=settings.QUESTION_FREQUENCY
            )
        message_controller = self.message_controllers[user_id]
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ MessageController
        context = {
            'recent_mood': 'neutral',
            'relationship_stage': 'introduction',
            'favorite_topics': [],
            'activity_level': 'moderate'
        }
        
        if memory:
            try:
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð½ÑÐ°Ð¹Ñ‚Ñ‹ Ð¾ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ðµ
                insights = await memory.get_conversation_insights()
                context.update({
                    'recent_mood': insights.get('recent_mood', 'neutral'),
                    'relationship_stage': insights.get('relationship_stage', 'introduction'),
                    'favorite_topics': insights.get('suggested_topics', []),
                    'activity_level': insights.get('activity_level', 'moderate')
                })
            except Exception as e:
                print(f"Warning: Could not get conversation insights: {e}")
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¾ÐºÑ€Ð°ÑÐºÑƒ
        enhanced_response = await message_controller.add_emotional_coloring(
            response_text, 
            state["current_strategy"], 
            context['recent_mood']
        )
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ MessageController Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
        processed = await message_controller.process_message(enhanced_response, context)
        
        state["processed_response"] = processed
        
        return state
    
    async def _persist(self, state: PipelineState) -> PipelineState:
        """Node 8: Persist conversation and update memory"""
        # Add assistant response to memory
        user_id = state["user_id"]
        memory = self.memory_instances.get(user_id)
        if memory:
            assistant_message = Message(
                role="assistant",
                content=" ".join(state["processed_response"]["parts"]),
                timestamp=datetime.utcnow(),
                metadata={
                    "strategy": state["current_strategy"],
                    "day_number": state["day_number"],
                    "has_question": state["processed_response"]["has_question"],
                    "processing_time_ms": int((datetime.utcnow() - state["processing_start"]).total_seconds() * 1000)
                }
            )
            context = MemoryContext(
                user_id=user_id,
                day_number=state["day_number"]
            )
            await memory.add_message(assistant_message, context)
        
        print(f"âœ… Persisted conversation for user {user_id}")
        return state
    
    def _split_response(self, text: str) -> List[str]:
        """Split response into 1-3 logical parts"""
        if len(text) <= settings.MAX_MESSAGE_LENGTH:
            return [text]
        
        # Try to split by sentences
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) <= 3:
            return sentences
        
        # Group sentences into 2-3 parts
        if len(sentences) <= 6:
            mid = len(sentences) // 2
            return [
                ". ".join(sentences[:mid]) + ".",
                ". ".join(sentences[mid:]) + "."
            ]
        else:
            third = len(sentences) // 3
            return [
                ". ".join(sentences[:third]) + ".",
                ". ".join(sentences[third:2*third]) + ".",
                ". ".join(sentences[2*third:]) + "."
            ]
    
    def _calculate_delays(self, parts: List[str]) -> List[int]:
        """Calculate typing delays between parts"""
        delays = [0]  # First part has no delay
        
        for i in range(1, len(parts)):
            # Simulate typing delay based on length
            chars = len(parts[i-1])
            typing_time = chars * 1000 // 50  # 50 chars per second
            delay = min(max(typing_time, 500), 3000)  # Between 0.5-3 seconds
            delays.append(delay)
        
        return delays 