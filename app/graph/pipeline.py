import asyncio
import os
import random
import re
from datetime import datetime
from typing import Dict, List, Any, Optional, TypedDict

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from ..memory.base import Message, MemoryContext
from ..memory.hybrid_memory import HybridMemory
from ..config.settings import settings
from ..utils.prompt_loader import PromptLoader
from ..utils.time_utils import TimeUtils
from ..utils.message_controller import MessageController
from ..utils.behavioral_analyzer import BehavioralAnalyzer
from ..utils.prompt_composer import PromptComposer

class PipelineState(TypedDict):
    
    user_id: str
    messages: List[Dict[str, Any]]
    meta_time: Optional[datetime]
    
    
    normalized_input: str
    memory_context: str
    day_prompt: str
    behavior_prompt: str
    final_prompt: str
    llm_response: str
    processed_response: Dict[str, Any]
    
    # Behavioral Analysis
    current_strategy: str
    behavioral_analysis: Dict[str, Any]
    strategy_confidence: float
    
    # Metadata
    day_number: int
    question_count: int
    processing_start: datetime

class AgathaPipeline:
 
    
    def __init__(self):
        self.prompt_loader = PromptLoader()
        self.time_utils = TimeUtils()
        self.memory_instances: Dict[str, HybridMemory] = {}
        
        self.message_controllers: Dict[str, MessageController] = {}
        self.behavioral_analyzer = BehavioralAnalyzer()
        self.prompt_composer = PromptComposer()
        
        # Initialize LLM - –ù–û–í–´–ô API
        api_key = os.getenv('OPENAI_API_KEY') or settings.OPENAI_API_KEY
        if api_key:
            print(f"üîë Using OpenAI API key: {api_key[:20]}...")
            self.llm = ChatOpenAI(
                api_key=api_key,
                model=settings.LLM_MODEL,
                temperature=0.8
            )
        else:
            print("‚ö†Ô∏è No OpenAI API key found, using Mock LLM")
            self.llm = None  
        
        
        self.graph = self._build_graph()
    
    def _build_graph(self):
        """Build the LangGraph pipeline - –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –Ω–æ–≤—ã–º API"""
        workflow = StateGraph(PipelineState)
        
        # Add nodes
        workflow.add_node("ingest_input", self._ingest_input)
        workflow.add_node("short_memory", self._short_memory)
        workflow.add_node("day_policy", self._day_policy)
        workflow.add_node("behavior_policy", self._behavior_policy)
        workflow.add_node("compose_prompt", self._compose_prompt)
        workflow.add_node("llm_call", self._llm_call)
        workflow.add_node("postprocess", self._postprocess)
        workflow.add_node("persist", self._persist)
        
        
        workflow.add_edge(START, "ingest_input")
        workflow.add_edge("ingest_input", "short_memory")
        workflow.add_edge("short_memory", "day_policy")
        workflow.add_edge("day_policy", "behavior_policy")
        workflow.add_edge("behavior_policy", "compose_prompt")
        workflow.add_edge("compose_prompt", "llm_call")
        workflow.add_edge("llm_call", "postprocess")
        workflow.add_edge("postprocess", "persist")
        workflow.add_edge("persist", END)
        
        return workflow.compile()
    
    async def process_chat(self, user_id: str, messages: List[Dict], meta_time: Optional[str] = None) -> Dict[str, Any]:
        """Main entry point for chat processing - –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"""
        state: PipelineState = {
            "user_id": user_id,
            "messages": messages,
            "meta_time": None,
            "normalized_input": "",
            "memory_context": "",
            "day_prompt": "",
            "behavior_prompt": "",
            "final_prompt": "",
            "llm_response": "",
            "processed_response": {},
            "current_strategy": "caring",
            "behavioral_analysis": {},
            "strategy_confidence": 0.0,
            "day_number": 1,
            "question_count": 0,
            "processing_start": datetime.utcnow()
        }
        
        if meta_time:
            try:
                state["meta_time"] = datetime.fromisoformat(meta_time.replace('Z', '+00:00'))
            except:
                state["meta_time"] = datetime.utcnow()
        else:
            state["meta_time"] = datetime.utcnow()
        
        
        result = await self.graph.ainvoke(state)
        return result["processed_response"]
    
    async def _fallback_pipeline(self, state: PipelineState) -> Dict[str, Any]:
        
        raise Exception("Fallback pipeline –Ω–µ –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ LangGraph.")
    
    async def _ingest_input(self, state: PipelineState) -> PipelineState:
        
        if not state["messages"]:
            state["normalized_input"] = ""
            return state
        
        
        user_messages = [msg for msg in state["messages"] if msg.get('role') == 'user']
        if user_messages:
            last_message = user_messages[-1]
            state["normalized_input"] = last_message.get('content', '').strip()
        
       
        state["day_number"] = 1  # TODO: Calculate from user profile
        
        return state
    
    async def _short_memory(self, state: PipelineState) -> PipelineState:
        """Node 2: Load and process short-term memory"""
        user_id = state["user_id"]
        
        # Get or create memory instance for user
        if user_id not in self.memory_instances:
            self.memory_instances[user_id] = HybridMemory(user_id)
        
        memory = self.memory_instances[user_id]
        
        # Add current message to memory
        if state["normalized_input"]:
            message = Message(
                role="user",
                content=state["normalized_input"],
                timestamp=state["meta_time"] or datetime.utcnow()
            )
            context = MemoryContext(
                user_id=user_id,
                day_number=state["day_number"]
            )
            await memory.add_message(message, context)
        
        # Get memory context
        state["memory_context"] = await memory.get_context(MemoryContext(
            user_id=user_id,
            day_number=state["day_number"]
        ))
        
        return state
    
    async def _day_policy(self, state: PipelineState) -> PipelineState:
        """Node 3: Apply daily scenario policy with enhanced context"""
        day_number = state["day_number"]
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–Ω–µ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
        state["day_prompt"] = await self.prompt_loader.get_day_prompt(day_number)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—Ä–µ–º–µ–Ω–∏ –∏ –∏—Å—Ç–æ—Ä–∏–∏
        user_id = state["user_id"]
        memory = self.memory_instances.get(user_id)
        
        if memory:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
                profile = await memory.get_user_profile()
                insights = await memory.get_conversation_insights()
                
                # –û–±–æ–≥–∞—â–∞–µ–º –ø—Ä–æ–º–ø—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                time_context = self.time_utils.get_time_context(state["meta_time"])
                
                enhanced_prompt = f"""
{state["day_prompt"]}

–ö–û–ù–¢–ï–ö–°–¢ –û–¢–ù–û–®–ï–ù–ò–ô:
- –°—Ç–∞–¥–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π: {insights.get('relationship_stage', 'introduction')}
- –£—Ä–æ–≤–µ–Ω—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {insights.get('personalization_level', 0):.1f}/1.0
- –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {profile.get('recent_mood', 'neutral')}
- –õ—é–±–∏–º—ã–µ —Ç–µ–º—ã: {', '.join([t[0] for t in profile.get('favorite_topics', [])[:3]])}

–í–†–ï–ú–ï–ù–ù–û–ô –ö–û–ù–¢–ï–ö–°–¢:
{time_context}

–ü–ê–ú–Ø–¢–¨ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{state["memory_context"][:200]}...
                """.strip()
                
                state["day_prompt"] = enhanced_prompt
                
            except Exception as e:
                print(f"Warning: Could not enhance day prompt: {e}")
        
        return state
    
    async def _behavior_policy(self, state: PipelineState) -> PipelineState:
        """Node 4: Advanced Behavioral Adaptation - –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        user_id = state["user_id"]
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        memory = self.memory_instances.get(user_id)
        user_profile = {}
        conversation_context = {}
        
        if memory:
            try:
                user_profile = await memory.get_user_profile()
                conversation_context = await memory.get_conversation_insights()
            except Exception as e:
                print(f"Warning: Could not get user profile: {e}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        behavioral_analysis = await self.behavioral_analyzer.analyze_user_behavior(
            messages=state["messages"],
            user_profile=user_profile,
            conversation_context=conversation_context
        )
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        recommended_strategy = behavioral_analysis['recommended_strategy']
        strategy_confidence = behavioral_analysis['strategy_confidence']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        state["current_strategy"] = recommended_strategy
        state["behavioral_analysis"] = behavioral_analysis
        state["strategy_confidence"] = strategy_confidence
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        print(f"üé≠ Behavioral Analysis for {user_id}:")
        print(f"   Emotion: {behavioral_analysis['dominant_emotion']} (intensity: {behavioral_analysis['emotional_intensity']:.2f})")
        print(f"   Communication: {behavioral_analysis['communication_style']}")
        print(f"   Needs: {', '.join(behavioral_analysis['relationship_needs'][:2])}")
        print(f"   Strategy: {recommended_strategy} (confidence: {strategy_confidence:.2f})")
        
        return state
    
    async def _compose_prompt(self, state: PipelineState) -> PipelineState:
        """Node 5: Advanced Prompt Composition with Behavioral Integration"""
        base_prompt = await self.prompt_loader.get_base_prompt()
        day_prompt = state["day_prompt"]
        strategy = state["current_strategy"]
        behavioral_analysis = state.get("behavioral_analysis", {})
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        time_context = self.time_utils.get_time_context(state["meta_time"])
        
        context_data = {
            'time_context': time_context,
            'memory_context': state["memory_context"],
            'user_message': state["normalized_input"],
            'max_length': settings.MAX_MESSAGE_LENGTH,
            'day_number': state["day_number"]
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º PromptComposer –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        state["final_prompt"] = await self.prompt_composer.compose_final_prompt(
            base_prompt=base_prompt,
            day_prompt=day_prompt,
            strategy=strategy,
            behavioral_analysis=behavioral_analysis,
            context_data=context_data
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–º–ø—Ç–∞
        print(f"üìù Prompt Composition:")
        print(f"   Strategy: {strategy}")
        print(f"   Day: {state['day_number']}")
        print(f"   User emotion: {behavioral_analysis.get('dominant_emotion', 'unknown')}")
        print(f"   Prompt length: {len(state['final_prompt'])} chars")
        print(f"   Max length setting: {settings.MAX_MESSAGE_LENGTH}")
        print(f"   Final prompt preview: {state['final_prompt'][:300]}...")
        
        return state
    
    async def _llm_call(self, state: PipelineState) -> PipelineState:
        """Node 6: Call LLM and get response"""
        if self.llm is None:
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π mock LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏
            user_input = state["normalized_input"]
            strategy = state["current_strategy"]
            day = state["day_number"]
            memory_context = state.get("memory_context", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –≤ –ø–∞–º—è—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ
            has_name = any(word in memory_context.lower() for word in ['–∑–æ–≤—É—Ç', '–∏–º—è', '–º–µ–Ω—è'])
            has_profession = any(word in memory_context.lower() for word in ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', 'team lead', '—Ä–∞–±–æ—Ç–∞—é'])
            has_location = any(word in memory_context.lower() for word in ['–∫–∏–µ–≤', '—Ö–∞—Ä—å–∫–æ–≤', '–æ–¥–µ—Å—Å–∞', '–ª—å–≤–æ–≤'])
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏
            user_name = ""
            if "–∑–æ–≤—É—Ç" in memory_context.lower():
                words = memory_context.split()
                for i, word in enumerate(words):
                    if word.lower() in ['–∑–æ–≤—É—Ç', '–∏–º—è'] and i + 1 < len(words):
                        user_name = words[i + 1].replace(',', '').replace('.', '')
                        break
            
            # –£–º–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            memory_questions = ["–ø–æ–º–Ω–∏—à—å", "–ø–æ–º–Ω–∏—Ç—å", "–∑–æ–≤—É—Ç", "–∏–º—è", "—Ä–∞–±–æ—Ç–∞", "—Å–µ–º—å—è", "–∂–∏–≤—É", "–≥–æ—Ä–æ–¥", "—É–≤–ª–µ—á–µ–Ω–∏—è", "—Ö–æ–±–±–∏"]
            is_memory_question = any(word in user_input.lower() for word in memory_questions)
            
            if is_memory_question:
                # –í–æ–ø—Ä–æ—Å—ã –æ –ø–∞–º—è—Ç–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                if memory_context and len(memory_context) > 50:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏
                    context_lower = memory_context.lower()
                    
                    # –ü–æ–∏—Å–∫ –∏–º–µ–Ω–∏
                    name_match = ""
                    if "–∑–æ–≤—É—Ç" in context_lower:
                        words = memory_context.split()
                        for i, word in enumerate(words):
                            if word.lower() in ['–∑–æ–≤—É—Ç', '–∏–º—è'] and i + 1 < len(words):
                                name_match = words[i + 1].replace(',', '').replace('.', '').replace(':', '')
                                break
                    
                    # –ü–æ–∏—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏
                    profession_match = ""
                    profession_words = ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "lead", "senior", "architect", "designer", "–º–µ–Ω–µ–¥–∂–µ—Ä"]
                    for word in profession_words:
                        if word in context_lower:
                            profession_match = word
                            break
                    
                    # –ü–æ–∏—Å–∫ –≥–æ—Ä–æ–¥–∞
                    city_match = ""
                    cities = ["–∫–∏–µ–≤", "–ª—å–≤–æ–≤", "—Ö–∞—Ä—å–∫–æ–≤", "–æ–¥–µ—Å—Å–∞", "–¥–Ω–µ–ø—Ä"]
                    for city in cities:
                        if city in context_lower:
                            city_match = city.capitalize()
                            break
                    
                    # –ü–æ–∏—Å–∫ –∫–æ–º–ø–∞–Ω–∏–∏
                    company_match = ""
                    if "softserve" in context_lower:
                        company_match = "SoftServe"
                    elif "–∫–æ–º–ø–∞–Ω–∏–∏" in context_lower:
                        company_match = "IT –∫–æ–º–ø–∞–Ω–∏–∏"
                    
                    # –ü–æ–∏—Å–∫ —Å–µ–º—å–∏
                    family_info = []
                    if "–∂–µ–Ω–∞" in context_lower or "–º—É–∂" in context_lower:
                        family_info.append("—Å–µ–º—å—è")
                    if "—Å—ã–Ω" in context_lower or "–¥–æ—á—å" in context_lower:
                        family_info.append("–¥–µ—Ç–∏")
                    
                    # –ü–æ–∏—Å–∫ —É–≤–ª–µ—á–µ–Ω–∏–π
                    hobbies = []
                    hobby_words = ["–≥–∏—Ç–∞—Ä", "–∞—Å—Ç—Ä–æ–Ω–æ–º", "—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ", "—Å–ø–æ—Ä—Ç", "—á–∏—Ç–∞—Ç—å", "–∏–≥—Ä"]
                    for hobby in hobby_words:
                        if hobby in context_lower:
                            hobbies.append(hobby)
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞
                    if "–∑–æ–≤—É—Ç" in user_input.lower() or "–∏–º—è" in user_input.lower():
                        if name_match:
                            response = f"–ö–æ–Ω–µ—á–Ω–æ –ø–æ–º–Ω—é! –¢–µ–±—è –∑–æ–≤—É—Ç {name_match}."
                        else:
                            response = "–•–º, –Ω–∞–ø–æ–º–Ω–∏ –º–Ω–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç?"
                    
                    elif "—Ä–∞–±–æ—Ç–∞" in user_input.lower() or "—Ä–∞–±–æ—Ç–∞—é" in user_input.lower():
                        if profession_match and company_match:
                            response = f"–¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å {profession_match}–æ–º –≤ {company_match}"
                            if city_match:
                                response += f" –≤ {city_match}–µ"
                            response += "."
                        elif profession_match:
                            response = f"–¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å {profession_match}–æ–º."
                        else:
                            response = "–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –æ —Å–≤–æ–µ–π —Ä–∞–±–æ—Ç–µ - —è —Ö–æ—á—É –∑–∞–ø–æ–º–Ω–∏—Ç—å!"
                    
                    elif "—Å–µ–º—å—è" in user_input.lower():
                        if family_info:
                            response = f"–£ —Ç–µ–±—è –µ—Å—Ç—å {', '.join(family_info)}."
                        else:
                            response = "–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –æ —Å–≤–æ–µ–π —Å–µ–º—å–µ!"
                    
                    elif "–≥–æ—Ä–æ–¥" in user_input.lower() or "–∂–∏–≤—É" in user_input.lower():
                        if city_match:
                            response = f"–¢—ã –∂–∏–≤–µ—à—å –≤ {city_match}–µ."
                        else:
                            response = "–í –∫–∞–∫–æ–º –≥–æ—Ä–æ–¥–µ —Ç—ã –∂–∏–≤–µ—à—å?"
                    
                    elif "—É–≤–ª–µ—á–µ–Ω–∏—è" in user_input.lower() or "—Ö–æ–±–±–∏" in user_input.lower():
                        if hobbies:
                            response = f"–¢–≤–æ–∏ —É–≤–ª–µ—á–µ–Ω–∏—è: {', '.join(hobbies)}."
                        else:
                            response = "–ö–∞–∫–∏–µ —É —Ç–µ–±—è —É–≤–ª–µ—á–µ–Ω–∏—è?"
                    
                    else:
                        response = f"–ü–æ–º–Ω—é! {memory_context.split('|')[1] if '|' in memory_context else memory_context[:100]}..."
                
                else:
                    response = "–•–º, –Ω–∞–ø–æ–º–Ω–∏ –º–Ω–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ - –º—ã –Ω–µ–¥–∞–≤–Ω–æ –∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å?"
            
            else:
                # –û–±—ã—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (–ë–ï–ó –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤)
                mock_responses = {
                    "caring": f"–ü—Ä–∏–≤–µ—Ç! –Ø —Ä–∞–¥–∞ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å üòä –¢—ã –Ω–∞–ø–∏—Å–∞–ª: '{user_input[:50]}...'. –Ø —Ö–æ—á—É –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–±—è.",
                    "supportive": f"–ü–æ–Ω–∏–º–∞—é —Ç–≤–æ–∏ —á—É–≤—Å—Ç–≤–∞. –¢—ã —Å–∫–∞–∑–∞–ª: '{user_input[:50]}...'. –Ø –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å —Ç–µ–±–µ.",
                    "mysterious": f"–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ... '{user_input[:50]}...' ü§î –ï—Å—Ç—å —á—Ç–æ-—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ–µ –≤ —Ç–æ–º, —á—Ç–æ —Ç—ã —Å–∫–∞–∑–∞–ª.",
                    "playful": f"–û–π-–æ–π! '{user_input[:50]}...' üòÑ –¢—ã —Ç–∞–∫–æ–π –∑–∞–±–∞–≤–Ω—ã–π! –õ—é–±–ª—é —Ç–∞–∫–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã.",
                    "reserved": f"–ü–æ–Ω–∏–º–∞—é. –¢—ã —Å–∫–∞–∑–∞–ª: '{user_input[:50]}...'. –≠—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è —Ç–æ—á–∫–∞ –∑—Ä–µ–Ω–∏—è.",
                    "intellectual": f"–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –º—ã—Å–ª–∏! –¢—ã –ø–æ–¥–µ–ª–∏–ª—Å—è: '{user_input[:50]}...'. –î–∞–≤–∞–π –æ–±—Å—É–¥–∏–º —ç—Ç–æ –≥–ª—É–±–∂–µ."
                }
                response = mock_responses.get(strategy, f"–°–ø–∞—Å–∏–±–æ –∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ! –°—Ç—Ä–∞—Ç–µ–≥–∏—è {strategy}.")
            
            # –í–ê–ñ–ù–û: Mock LLM –ù–ï –¥–æ–±–∞–≤–ª—è–µ—Ç –≤–æ–ø—Ä–æ—Å—ã - —ç—Ç–æ –¥–µ–ª–∞–µ—Ç MessageController
            state["llm_response"] = response
        else:
            try:
                # –†–µ–∞–ª—å–Ω—ã–π API OpenAI
                print(f"ü§ñ Calling OpenAI API with prompt length: {len(state['final_prompt'])}")
                response = await self.llm.ainvoke([HumanMessage(content=state["final_prompt"])])
                state["llm_response"] = response.content.strip()
                print(f"‚úÖ OpenAI response length: {len(state['llm_response'])} chars")
                print(f"üìù Response preview: {state['llm_response'][:200]}...")
            except Exception as e:
                print(f"‚ùå LLM call failed: {e}")
                state["llm_response"] = "–ò–∑–≤–∏–Ω–∏, —É –º–µ–Ω—è —Å–µ–π—á–∞—Å –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π. –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑?"
        
        return state
    
    async def _postprocess(self, state: PipelineState) -> PipelineState:
        """Node 7: Post-process response (split, add delays, manage questions)"""
        response_text = state["llm_response"]
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        user_id = state["user_id"]
        memory = self.memory_instances.get(user_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º MessageController –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_id not in self.message_controllers:
            self.message_controllers[user_id] = MessageController(
                max_message_length=settings.MAX_MESSAGE_LENGTH,
                question_frequency=settings.QUESTION_FREQUENCY
            )
        message_controller = self.message_controllers[user_id]
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è MessageController
        context = {
            'recent_mood': 'neutral',
            'relationship_stage': 'introduction',
            'favorite_topics': [],
            'activity_level': 'moderate'
        }
        
        if memory:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å–∞–π—Ç—ã –æ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ
                insights = await memory.get_conversation_insights()
                context.update({
                    'recent_mood': insights.get('recent_mood', 'neutral'),
                    'relationship_stage': insights.get('relationship_stage', 'introduction'),
                    'favorite_topics': insights.get('suggested_topics', []),
                    'activity_level': insights.get('activity_level', 'moderate')
                })
            except Exception as e:
                print(f"Warning: Could not get conversation insights: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É
        enhanced_response = await message_controller.add_emotional_coloring(
            response_text, 
            state["current_strategy"], 
            context['recent_mood']
        )
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MessageController –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processed = await message_controller.process_message(enhanced_response, context)
        
        state["processed_response"] = processed
        
        return state
    
    async def _persist(self, state: PipelineState) -> PipelineState:
        """Node 8: Persist conversation and update memory"""
        # Add assistant response to memory
        user_id = state["user_id"]
        memory = self.memory_instances.get(user_id)
        if memory:
            assistant_message = Message(
                role="assistant",
                content=" ".join(state["processed_response"]["parts"]),
                timestamp=datetime.utcnow(),
                metadata={
                    "strategy": state["current_strategy"],
                    "day_number": state["day_number"],
                    "has_question": state["processed_response"]["has_question"],
                    "processing_time_ms": int((datetime.utcnow() - state["processing_start"]).total_seconds() * 1000)
                }
            )
            context = MemoryContext(
                user_id=user_id,
                day_number=state["day_number"]
            )
            await memory.add_message(assistant_message, context)
        
        print(f"‚úÖ Persisted conversation for user {user_id}")
        return state
    
    def _split_response(self, text: str) -> List[str]:
        """Split response into 1-3 logical parts"""
        if len(text) <= settings.MAX_MESSAGE_LENGTH:
            return [text]
        
        # Try to split by sentences
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) <= 3:
            return sentences
        
        # Group sentences into 2-3 parts
        if len(sentences) <= 6:
            mid = len(sentences) // 2
            return [
                ". ".join(sentences[:mid]) + ".",
                ". ".join(sentences[mid:]) + "."
            ]
        else:
            third = len(sentences) // 3
            return [
                ". ".join(sentences[:third]) + ".",
                ". ".join(sentences[third:2*third]) + ".",
                ". ".join(sentences[2*third:]) + "."
            ]
    
    def _calculate_delays(self, parts: List[str]) -> List[int]:
        """Calculate typing delays between parts"""
        delays = [0]  # First part has no delay
        
        for i in range(1, len(parts)):
            # Simulate typing delay based on length
            chars = len(parts[i-1])
            typing_time = chars * 1000 // 50  # 50 chars per second
            delay = min(max(typing_time, 500), 3000)  # Between 0.5-3 seconds
            delays.append(delay)
        
        return delays 